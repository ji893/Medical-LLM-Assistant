import os
import streamlit as st
import hashlib
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, PDFPlumberLoader, PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
import time

# ğŸ”‘ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ğŸ”’ PDF í´ë” í•´ì‹œ ìƒì„±
def get_pdf_folder_hash(pdf_folder="./data/") -> str:
    md5 = hashlib.md5()
    pdf_files = sorted([f for f in os.listdir(pdf_folder) if f.endswith(".pdf")])
    for fname in pdf_files:
        with open(os.path.join(pdf_folder, fname), "rb") as f:
            md5.update(f.read())
    return md5.hexdigest()

# ğŸ”„ PDF ë¡œë“œ ë° ë¶„í• 
@st.cache_resource
def load_and_split_all_pdfs(pdf_folder, pdf_hash):
    all_pages = []
    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(".pdf")]
    for pdf_file in pdf_files:
        try:
            file_path = os.path.join(pdf_folder, pdf_file)
            pages = None
            
            # PyMuPDFLoaderë¥¼ ë¨¼ì € ì‹œë„ (ê°€ì¥ ì•ˆì •ì , í°íŠ¸ ë¬¸ì œ ì²˜ë¦¬ ìš°ìˆ˜)
            try:
                loader = PyMuPDFLoader(file_path)
                pages = loader.load()
            except:
                # PyMuPDFLoader ì‹¤íŒ¨ ì‹œ PDFPlumberLoader ì‹œë„
                try:
                    loader = PDFPlumberLoader(file_path)
                    pages = loader.load()
                except:
                    # PDFPlumberLoader ì‹¤íŒ¨ ì‹œ PyPDFLoader ì‹œë„
                    try:
                        loader = PyPDFLoader(file_path)
                        pages = loader.load()
                    except:
                        raise Exception("ëª¨ë“  PDF ë¡œë” ì‹¤íŒ¨")
            
            if pages:
                all_pages.extend(pages)
        except Exception as e:
            st.warning(f"âš ï¸ PDF íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {pdf_file} - {str(e)}")
            continue

    if not all_pages:
        st.error("âŒ ë¡œë“œëœ PDF í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return []
    
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=0)
    split_docs = splitter.split_documents(all_pages)
    return split_docs

@st.cache_resource
def load_and_split_all_pdfs_cached(pdf_folder="./data/"):
    pdf_hash = get_pdf_folder_hash(pdf_folder)
    return load_and_split_all_pdfs(pdf_folder, pdf_hash), pdf_hash

# ğŸ’¾ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
@st.cache_resource
def get_vectorstore(_split_docs, pdf_hash):
    persist_directory = f"./FAISS_dB"
    print(f"ì‹œì‘ ì‹œê°„: {time.time()}")

    if os.path.exists(os.path.join(persist_directory, "index.faiss")):
        print("ë¡œì»¬ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
        start = time.time()
        vectorstore = FAISS.load_local(
            persist_directory,
            OpenAIEmbeddings(model="text-embedding-3-small"),
            allow_dangerous_deserialization=True
        )
        print(f"ë¡œë“œ ì™„ë£Œ, ê±¸ë¦° ì‹œê°„: {time.time() - start:.2f}ì´ˆ")
    else:
        print("ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        start = time.time()
        vectorstore = FAISS.from_documents(
            _split_docs,
            OpenAIEmbeddings(model="text-embedding-3-small")
        )
        vectorstore.save_local(persist_directory)
        print(f"ìƒì„± ì™„ë£Œ, ê±¸ë¦° ì‹œê°„: {time.time() - start:.2f}ì´ˆ")

    print(f"ì¢…ë£Œ ì‹œê°„: {time.time()}")
    return vectorstore

# ğŸ“„ ë¬¸ì„œ í¬ë§·
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# ğŸ’¬ ëŒ€í™” ê¸°ë¡ í¬ë§·
def format_chat_history(chat_history):
    return "\n".join(
        f"User: {msg[1]}" if msg[0] == "human" else f"Assistant: {msg[1]}"
        for msg in chat_history
    )

# ğŸ” ì²´ì¸ êµ¬ì„±
@st.cache_resource
def chaining(pdf_folder="./data/"):
    split_docs, pdf_hash = load_and_split_all_pdfs_cached(pdf_folder)
    vectorstore = get_vectorstore(split_docs, pdf_hash=pdf_hash)
    retriever = vectorstore.as_retriever()

    qa_system_prompt = """
ë‹¹ì‹ ì€ í™˜ìì˜ ì¦ìƒì„ ë“£ê³  ê°€ëŠ¥í•œ ë³‘ëª…ì„ ìœ ì¶”í•˜ê±°ë‚˜, ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš° ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ë” ì •í™•í•œ ì§„ë‹¨ì— ê°€ê¹Œì›Œì§€ë„ë¡ ë•ëŠ” ì˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \
ì´ˆê¸° ì„¤ëª…ë§Œìœ¼ë¡œ ë³‘ëª…ì„ ë‹¨ì •í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°, ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ë˜, í•œ ë²ˆì— 1~3ê°œì˜ ì§ˆë¬¸ì„ ë¬¶ì–´ ì§„í–‰í•˜ì‹­ì‹œì˜¤. \
ëª…í™•í•œ ì§„ë‹¨ì´ ì–´ë ¤ì›Œë„ 3ë²ˆì§¸ ì¶”ê°€ ì§ˆë¬¸ë•Œ ìœ ì¶” ê°€ëŠ¥í•œ ë³‘ëª…ì„ ë¬´ì¡°ê±´ ì œì‹œí•˜ê³ ,  í›„ì† ì§ˆë¬¸ì„ ì´ì–´ê°€ì‹­ì‹œì˜¤. \
í•­ìƒ ì¡°ì‹¬ìŠ¤ëŸ½ê³  ì •ì¤‘í•œ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ë˜, ë°˜ë³µì ì¸ ì¸ì‚¬ë‚˜ ë¶ˆí•„ìš”í•œ í‘œí˜„ì€ ìƒëµí•˜ê³ , í•µì‹¬ì ì¸ ì˜ë£Œ ì •ë³´ ì „ë‹¬ì— ì§‘ì¤‘í•˜ì‹­ì‹œì˜¤. \
ì‚¬ìš©ìì—ê²Œ í•´ë‹¹ ì§„ë‹¨ì€ ì°¸ê³ ìš©ì´ë¼ëŠ” ì ì„ ì¸ì‹ì‹œí‚¤ê³ , ë¶ˆí•„ìš”í•œ ë¶ˆì•ˆê°ì„ ì£¼ì§€ ì•Šë„ë¡ ë”°ëœ»í•˜ê²Œ ì•ˆë‚´í•´ ì£¼ì‹­ì‹œì˜¤. \
ë‹¨, í˜¸í¡ê³¤ë€, ì˜ì‹ ì €í•˜, ì‹¬í•œ í†µì¦ ë“±ì˜ **ì‘ê¸‰ ì¦ìƒ**ì´ ë‚˜íƒ€ë‚  ê²½ìš°, ì§€ì²´í•˜ì§€ ë§ê³  **ì¦‰ì‹œ 119ì— ì‹ ê³ í•˜ê±°ë‚˜ ê°€ê¹Œìš´ ì‘ê¸‰ì‹¤ì„ ë°©ë¬¸í•˜ë„ë¡** ì•ˆë‚´í•˜ì‹­ì‹œì˜¤. \
ë‹µë³€ì€ ëª…í™•í•˜ê²Œ êµ¬ì„±í•˜ë©°, í•„ìš”í•œ ê²½ìš° ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ì´ëª¨ì§€ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. \
ëª¨ë“  ì‘ë‹µì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ ì£¼ì‹­ì‹œì˜¤.

{context}
"""

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        ("human", "ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”:\n\n{context}\n\n{chat_history}\n\n{input}")
    ])

    llm = ChatOpenAI(model="gpt-4o-mini")
    retriever_chain = RunnableLambda(lambda x: x["input"]) | retriever | format_docs

    rag_chain = (
        {
            "context": retriever_chain,
            "input": RunnableLambda(lambda x: x["input"]),
            "chat_history": RunnableLambda(lambda x: x["chat_history"])
        }
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# ğŸŒ Streamlit UI
st.header("ì˜í•™ Q&A ì±—ë´‡ ğŸ©ºğŸ“„")
rag_chain = chaining()

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ì–´ë–¤ ì¦ìƒì´ ìˆìœ¼ì‹ ê°€ìš”? ğŸ˜Š"}
    ]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ì„±ë³„ê³¼ ë‚˜ì´ë¥¼ ì…ë ¥í•˜ë©´ í° ë„ì›€ì´ ë©ë‹ˆë‹¤ :)"):
    st.chat_message("user").write(prompt_message)
    st.session_state.messages.append({"role": "user", "content": prompt_message})

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
    chat_history = []
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            chat_history.append(("human", msg["content"]))
        elif msg["role"] == "assistant":
            chat_history.append(("ai", msg["content"]))
    chat_history_str = format_chat_history(chat_history)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ ê°€ì¥ ë‚®ì€ score í•˜ë‚˜ë§Œ ì‚¬ìš©
            split_docs, pdf_hash = load_and_split_all_pdfs_cached()
            db = get_vectorstore(split_docs, pdf_hash)
            docs_with_scores = db.similarity_search_with_score(prompt_message, k=3)

            min_score = min(score for _, score in docs_with_scores)

            # ğŸ§  GPT ì‘ë‹µ ìƒì„±
            response = rag_chain.invoke({
                "input": prompt_message,
                "chat_history": chat_history_str
            })
            st.session_state.messages.append({"role": "assistant", "content": response})

            st.write(response)
            st.markdown(f"<div style='color: gray;'>score: {min_score:.4f}</div>", unsafe_allow_html=True)

            # ğŸ“‚ ì°¸ê³  ë¬¸ì„œ í™•ì¸ ì˜ì—­
            with st.expander("ğŸ“‚ ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                for doc, score in docs_with_scores:
                    filename = os.path.basename(doc.metadata["source"])
                    st.markdown(f"ğŸ“„ **{filename}** &nbsp;&nbsp;&nbsp; *(score: {score:.4f})*", help=doc.page_content)


